{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final exam practice questions\n",
    "\n",
    "#### Question 1\n",
    "In this question, we will create a function to generate linear data step-by-step. \n",
    "1. Define a function called `gen_lin_data` that takes as its input an integer `n`, and returns a matrix `X` and a vector `Y`. \n",
    "2. First we will generate `X`. Within the function,\n",
    " - Create two random (numpy) vectors of size `n` named `X_1` and `X_2` where \n",
    "    - Each element of `X_1` is independent and distributed according to the Uniform(-2,2) distribution\n",
    "    - Each element `X_2` is independent and distributed according to the Normal(2,2) distribution. \n",
    "    \n",
    "  - Finally, concatenate `X_1` and `X_2` to generate `X` so that `X` is a numpy object with `n` rows and 2 columns.\n",
    "\n",
    "\n",
    "3. Now we will work towards generating `Y`. Within the function,\n",
    "  - Define a numpy array called `beta` and set it equal to `[-1,2]`. \n",
    "  - Define another `n` dimensional vector called `eps` where each element is independent and distributed according to the Normal(0,1/2) distribution. \n",
    "  - Finally, Generate a vector of size `n` called `Y` according to the following equation:\n",
    "$$\n",
    "Y = X \\cdot \\beta + eps\n",
    "$$\n",
    "4. Now that your function is defined, test your function in the designated cell and call it with **`n` equal to 500**. Save the resulting arrays as `Y500` and `X500`. **Print** the average of `Y500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function here\n",
    "def gen_lin_data(n):\n",
    "    X_1 = np.random.uniform(-2,2,n)\n",
    "    X_2 = np.random.normal(2,2, n)\n",
    "    X = np.vstack([X_1,X_2]).T\n",
    "    beta = np.array([-1,2])\n",
    "    eps = np.random.normal(0,1/2,n)\n",
    "   \n",
    "    Y = X @ beta + eps\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.803125339392462\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10) #ignore this line and do not delete it\n",
    "\n",
    "# Define Y500 and X500 below this line:\n",
    "X500, Y500 = gen_lin_data(500)\n",
    "# Your code here\n",
    "print(np.mean(Y500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "In this question, we will create our own function to perform basic OLS estimation.\n",
    "1. Define a function called `simple_OLS` that takes a vector `Y` and a matrix `X` as inputs and returns a vector of OLS (linear regression) coefficient estimates. **To get full marks, you must explicitly calculate the OLS coefficients using the analytical formula for linear regression.** You will get some partial credit for estimating the equations using another method.\n",
    "2. **In the Markdown Cell below your function**, answer this question: If `y` is an $n$-vector and `X` is an $n\\times d$ matrix, what are the dimensions of the objects that will be output by the `simple_OLS` function? Verify this with your code or justify it using linear algebra.\n",
    "3. In the designated cell, define `beta_hat` to be the output of `simple_OLS` using `Y500` and `X500` from the previous problem.\n",
    "4. In the second provided Markdown Cell, explain what values you would expect the entries of `beta_hat` to be close to, and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function here\n",
    "def simple_OLS(Y, X):\n",
    "    return np.linalg.inv(X.T @ X)@(X.T @ Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 1\n",
    "The simple OLS function will return a $d$-vector (or column vector). You can see this by calling `beta_hat.shape`(with $d = 2$) or by noting that it will share a row dimension with `X.t` (which is $d$) and a column dimension with `Y` (which is 1), making it a $d$-vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.98368016  1.99845442]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# Define beta_hat here\n",
    "beta_hat = simple_OLS(Y500, X500)\n",
    "print(beta_hat)\n",
    "print(beta_hat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 2\n",
    "We would expect beta_hat to be close to `[-1, 2]` because that was the value of the true linear coefficients we generated the data with. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "In this question, we will evaluate the performance of our OLS estimator. \n",
    "1. Define a function called `simple_eval` that takes `X`, `Y`, and `beta_hat` as inputs and returns `est_mse`, the mean-squared error of `beta_hat`. \n",
    "2. Within that function\n",
    "    - Define `Yhat` to be the predicted values of `beta_hat` appled to `X`. \n",
    "    - Calculate the mean-squared error and set it to `est_mse`. To do this, you can either use the metrics module from scikitlearn or calculate the mean-squared error by hand. \n",
    "3. In the cell below your function, call your `simple_eval` function using `X500`, `Y500`, and `beta_hat` from Question 2. Save that value to a variable called `ols_mse`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function here\n",
    "def simple_eval(X, Y, beta_hat):\n",
    "    Yhat = X @ beta_hat\n",
    "    est_mse = sum(np.power(Y - Yhat,2))/len(Y)\n",
    "    #est_mse = metrics.mean_squared_error(Y500, Yhat)\n",
    "    return est_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23351289190033098"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define ols_mse here\n",
    "ols_mse = simple_eval(X500, Y500, beta_hat)\n",
    "ols_mse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Now, fit a single regression tree on `Y500` and `X500`. Set the `random_state` and `max_depth` arguments equal to 123 and 10 respectively. Calculate the resulting training mean-squared error of this tree and set it equal to `tree_mse`.\n",
    "\n",
    "Is the MSE from this regression tree lower or higher than the mean-squared error from linear regression (OLS)? Why would that be? Enter your answer in the markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018038859089902624\n"
     ]
    }
   ],
   "source": [
    "# Fit Tree Here\n",
    "from sklearn import tree, metrics\n",
    "reg_tree = tree.DecisionTreeRegressor(random_state=123, max_depth = 10).fit(y = Y500, X = X500)\n",
    "tree_mse = metrics.mean_squared_error(Y500,reg_tree.predict(X500))\n",
    "print(tree_mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response\n",
    "The MSE for the regression tree is lower. This is because it is highly over parameterized (overfitted) to the training data we are evaluating it on. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Now we will generate test data by using `gen_lin_data` with `n` equal to 200. Save the resulting arrays as `X200` and `Y200`. Using `simple_eval` and this testing data, calculate the testing mean-squared error of the `beta_hat` estimate from Question 2. Do the same for tree estimator from Question 4. In the Markdown cell below, answer the following question: Which estimator has a lower test mean-squared error? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25686370013802284\n",
      "0.5989805366324684\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Your code here\n",
    "X200, Y200 = gen_lin_data(200)\n",
    "mse_test_ols = simple_eval(X200, Y200, beta_hat)\n",
    "print(mse_test_ols)\n",
    "tree_mse_test = metrics.mean_squared_error(Y200,reg_tree.predict(X200))\n",
    "print(tree_mse_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response\n",
    "OLS has a lower (better) test mse. This is because the tree model was overfitting, and now we are predicting out-of-sample.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

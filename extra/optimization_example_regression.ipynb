{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization example: Linear regression\n",
    "\n",
    "In this example, we will solve a linear regression problem three different ways.\n",
    "\n",
    "Before we start, we need to define the `mean_square_error` objective function, and provide the \"data\" matrix $X$ and the \"outcome\" vector $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6e3b2-52c0-402e-aa52-f5774aa45157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "import scipy.optimize as opt\n",
    "\n",
    "\n",
    "def mean_square_error(X,y,beta):\n",
    "    mse = np.linalg.norm((X @ beta) - y) **2\n",
    "    return mse\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([7,8,9])\n",
    "\n",
    "mse_for_this_problem = lambda beta: mean_square_error(X,y,beta) # define a specific version of the mean_square_error function that uses X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc9183-3587-41a5-80ac-88433f8fb392",
   "metadata": {},
   "source": [
    "First, we will minimize the objective function numerically, using `scipy.optimize.minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173d351-bcc8-462d-9b17-d656389868f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scipy = opt.minimize(mse_for_this_problem, np.zeros(2)).x #provide an initial guess of beta = [0,0]\n",
    "print(reg_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will solve the problem analytically, using the closed form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb899998-aaa9-42dc-833c-393c296a043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_inv = np.linalg.inv(X.T @ X) @ X.T @ y # this is the analytical solution\n",
    "reg_solve = np.linalg.solve(X.T @ X, X.T @ y) # this is also the analytical solution, but should be faster than using the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb64f9-a219-4d40-b1f2-40a4a1378ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"reg_inv = {reg_inv}\") # notice that they give the same answer\n",
    "print(f\"beta_hat3 = {reg_solve}\")  # The time difference won't be noticeable for small matrices like this one, but it will be substantial when you have more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a45bf9-c271-4a71-b53e-c45eae4f81f2",
   "metadata": {},
   "source": [
    "Finally, we'll solve the same problem using scikit-learn and statsmodels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.linear_model\n",
    "reg_sklearn = linear_model.LinearRegression(fit_intercept=False)\n",
    "reg_sklearn.fit(X, y)\n",
    "beta_hat_sklearn = reg_sklearn.coef_\n",
    "print(f\"beta_hat_sklearn = {beta_hat_sklearn}\")\n",
    "\n",
    "# Using statsmodels.api\n",
    "reg_statsmodels = sm.OLS(y, X)\n",
    "sm_result = reg_statsmodels.fit()\n",
    "beta_hat_statsmodels = sm_result.params\n",
    "print(f\"beta_hat_statsmodels = {beta_hat_statsmodels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At face value, it is good that all methods give the same answer. However, it is good to note that\n",
    "\n",
    "1. The numerical solution is not exact, will depend on the initial guess, and may not converge. However, the numerical solution is very general, and can be used for nonlinear models.\n",
    "3. The analytical solution is exact, but only works for linear regression. Using `np.linalg.solve` will be faster than inverting the matrix.\n",
    "4. The scikit-learn and statsmodels solutions are also exact, and are optimized for linear regression. They may be slower than the analytical solution, but give you a lot of extra important information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
